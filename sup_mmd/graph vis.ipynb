{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sup_mmd.data import MMD_Dataset\n",
    "import sys, json, re, os, glob, shutil\n",
    "from commons.utils import get_logger\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sup_mmd.model import LinearModel1, mmd_loss_pq, LinearModelComp1, mmd_loss_pq_comp\n",
    "from sup_mmd.model import MMD, MMD_comp\n",
    "from submodular.maximize import greedy as greedy_maximize\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from sup_mmd.functions import softmax, nz_median_dist, combine_kernels\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "logger = get_logger(\"Infer\")\n",
    "GPU_MODE = False\n",
    "import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_generic = re.compile(r'(mmdpq?)(_)(.?)(duc03|duc04|tac08|tac09)-([AB])_([xc])\\.(\\d+|x)_g(\\d\\.?\\d*)_b(\\d\\.?\\d*)_a(\\d\\.?\\d*)(_)SF(b|x)(k|x)(c|x)') \n",
    "pattern_update = re.compile(r'(mmdpq?)-comp([01])\\.(lin1|lin2)_(tac08|tac09)-([AB])_([xc])\\.(\\d+|x)_g(\\d\\.?\\d*)_b(\\d\\.?\\d*)_a(\\d+)_l(\\d\\.?\\d*)_SF(b|x)(k|x)(c|x)')   \n",
    "\n",
    "TARGET_NAME = \"y_hm_0.4\"\n",
    "BUDGET = 125 ##aftre compression, it will be less\n",
    "#### ROUGE eval truncates, so > 100 words will be truncated\n",
    "\n",
    "CACHE_ROOT = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model_path, r, group):\n",
    "    model_file = model_path.split(\"/\")[-1]\n",
    "    s = pattern_generic.search(model_file)\n",
    "    generic = True ## MMD() or MMD() - lambda*MMD()\n",
    "    if not s:\n",
    "        s = pattern_update.search(model_file)\n",
    "        if not s:\n",
    "            logger.error(\"pattern not matched with both generic/update regexes, quitting \" + model_file)\n",
    "            return\n",
    "        generic = False\n",
    "\n",
    "    name = s.group(0)\n",
    "    loss_name = s.group(1)\n",
    "\n",
    "    assert loss_name in {\"mmdpq\"}\n",
    "    train_dataset = s.group(4).lower()\n",
    "    assert train_dataset in {\"duc03\", \"duc04\", \"tac08\", \"tac09\"}\n",
    "    set_ = s.group(5)\n",
    "    assert set_ in {\"A\", \"B\"}\n",
    "    if not generic or set_ == \"B\":\n",
    "        assert train_dataset in {\"tac08\", \"tac09\"}\n",
    "    compress = s.group(6) == \"c\"\n",
    "    split_seq = s.group(7)\n",
    "    \n",
    "    if split_seq != \"x\":\n",
    "        logger.warning(\"please supply retrained model\")\n",
    "\n",
    "    gamma1 = float(s.group(8))\n",
    "    beta = float(s.group(9))\n",
    "    alpha_seq = s.group(10)\n",
    "    lambdaa = 0.0\n",
    "\n",
    "    lambdaa, diff, model_name = s.group(11), s.group(2), s.group(3)\n",
    "    \n",
    "    if not generic:\n",
    "        assert set_ == \"B\"\n",
    "        lambdaa = float(s.group(11))\n",
    "        diff = int(s.group(2))\n",
    "        model_name = s.group(3)\n",
    "    \n",
    "    BOOST_FIRST = s.group(12) == \"b\"\n",
    "    KEYWORDS = s.group(13) == \"k\"\n",
    "    comp_feats = s.group(14) ==\"c\"\n",
    "    \n",
    "    logger.debug((name, list(s.groups()), (loss_name, diff, model_name, train_dataset, set_, compress, split_seq, gamma1, beta, alpha_seq, lambdaa, BOOST_FIRST, KEYWORDS, comp_feats) ) )\n",
    "    \n",
    "    dataset = {\n",
    "        \"duc03\": \"duc04\", \n",
    "        \"duc04\": \"duc03\",\n",
    "        \"tac08\": \"tac09\",\n",
    "        \"tac09\": \"tac08\"\n",
    "    }[train_dataset]\n",
    "\n",
    "    dataset_name = \"{}_{}\".format(dataset, TARGET_NAME)\n",
    "\n",
    "    logger.debug(\"loading data from \" + dataset_name)\n",
    "    data = MMD_Dataset.load(dataset_name, CACHE_ROOT, compress = compress)\n",
    "    SURF_IDXS = data.surf_idxs(keywords = KEYWORDS, boost_first = BOOST_FIRST, comp = ( comp_feats and set_ == \"B\" ) )\n",
    "    \n",
    "    logger.info(\"surf feats: {}\".format(\n",
    "        \",\".join( np.array(data.surf_names)[SURF_IDXS] )\n",
    "    ))\n",
    "\n",
    "    logger.debug(\"loading model from \" + model_path)\n",
    "    try:\n",
    "        model, alpha, train_idxs, val_idxs, epochs = LinearModel1.load(len(SURF_IDXS), model_path)\n",
    "    except:\n",
    "        model, alpha, train_idxs, val_idxs, epochs = LinearModelComp1.load(len(SURF_IDXS), len(SURF_IDXS), model_path)\n",
    "\n",
    "    idxs = np.arange( len(data) ).tolist()\n",
    "\n",
    "    root = \"./{}_{}/\".format( dataset, set_ )\n",
    "\n",
    "    if not os.path.exists(root + \"summaries\"):\n",
    "        try:\n",
    "            os.makedirs(root + \"summaries\")\n",
    "        except:\n",
    "            pass\n",
    "#     shutil.copy2( model_path, root )\n",
    "\n",
    "    logger.debug(\"Dataset and model loaded, begin inference with #topics={}, generic?={}\".format( len(idxs), generic ))\n",
    "\n",
    "    if GPU_MODE:\n",
    "        model.cuda()\n",
    "\n",
    "    ix = np.where(np.array(data.groups) == group)[0][0]\n",
    "#     print(ix)\n",
    "    # group = data.groups[ix]            \n",
    "    subset = data.get_subset_df(group, set_ )\n",
    "    write_df = subset[[\"position\", \"doc_sents\", \"sent_id\", \"group\", \"target\", \"set\", \"doc_id\", \"num_words\", \"R1.R\", \"R1.P\", \"R2.R\", \"R2.P\", \"nouns\", \"prpns\"]]\n",
    "    surf_names = np.array(data.surf_names)[SURF_IDXS]\n",
    "\n",
    "    if generic:\n",
    "        if train_dataset in [\"duc03\", \"duc04\"]:\n",
    "            K, X, _, _ = data[ix]\n",
    "        elif train_dataset in [\"tac08\", \"tac09\"]:\n",
    "            if set_ == \"A\":\n",
    "                # logger.info(\"A\")\n",
    "                K, _, _, X, _, _, _, _, _ = data[ix]\n",
    "            else:\n",
    "                # logger.info(\"B\")\n",
    "                _, K, _, _, X, _, _, _, _ = data[ix]\n",
    "        \n",
    "        K, X = K.squeeze(), X.squeeze()[:, SURF_IDXS]\n",
    "        fg = model.forward( X )[0]\n",
    "        K_combined = combine_kernels(K, alpha, gamma1) \n",
    "        mmd = MMD(K_combined, fg)\n",
    "        K = torch.einsum('ijk,k->ij', K, alpha)\n",
    "\n",
    "    else:\n",
    "        KA, KB, KAB, XA, X, _, _, _, _ = data[ix]\n",
    "        KA, XA = KA.squeeze(), XA.squeeze()[:, SURF_IDXS]\n",
    "        KB, X = KB.squeeze(), X.squeeze()[:, SURF_IDXS]\n",
    "        KAB = KAB.squeeze()\n",
    "        \n",
    "        fA, fg = model.forward( XA, X )\n",
    "        KA_combined = combine_kernels(KA, alpha, gamma1)  \n",
    "        KB_combined = combine_kernels(KB, alpha, gamma1)  \n",
    "        KAB_combined = combine_kernels(KAB, alpha, gamma1)\n",
    "        mmd = MMD_comp( KB_combined, KA_combined, KAB_combined, fg, fA, lambdaa = lambdaa, diff = diff)\n",
    "        K = torch.einsum('ijk,k->ij', KB, alpha)\n",
    "\n",
    "    write_df[\"nf\"] = X[:, np.where(surf_names==\"nf\")[0]] #normalised\n",
    "    write_df[\"lexrank\"] = X[:, np.where(surf_names==\"lexrank\")[0]].numpy() + 1.0\n",
    "    \n",
    "    write_df[\"tfisf\"] = X[:, np.where(surf_names==\"tfisf\")[0]].numpy() ## normalised\n",
    "    write_df[\"btfisf\"] = X[:, np.where(surf_names==\"btfisf\")[0]].numpy() #normalised\n",
    "    write_df[\"scores\"] = softmax(fg.detach().numpy()) * len(subset)\n",
    "\n",
    "    lengths = subset[\"num_words\"].values\n",
    "    keys = None\n",
    "    if compress:\n",
    "        keys = [int(sid.split(\"-\")[0]) for sid in subset[\"sent_id\"]]\n",
    "    S, cost = greedy_maximize(mmd, budget = BUDGET, \n",
    "                costs = copy(lengths), r = r, verbose = False, keys = keys)\n",
    "    K = K.numpy()\n",
    "    np.fill_diagonal(K, 0.0)\n",
    "    return K, write_df, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "May-27 00:33:29 INFO [data:531]=> loaded from ./data//tac09_y_hm_0.4.pik\n",
      "May-27 00:33:29 INFO [Infer:61]=> surf feats: rel_pos,pos1,pos2,pos3,pos4+,#words,par_start,#nouns,query_sim,tfisf,btfisf,lexrank\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 192 0.09989594172736732 5\n"
     ]
    }
   ],
   "source": [
    "# group, T = \"D0910\", 0.15\n",
    "# group, T = \"D0908\", 0.1\n",
    "group, T = \"D0929\", 0.1\n",
    "K, df, S = infer(\"tac09_A/mmdpq_tac08-A_x.x_g2.25_b0.08_a0_SFbxx.net\", 0.01, group)\n",
    "print ( K.shape[0], (K > T).sum() // 2, (K > T).sum() / ( K.shape[0] * K.shape[1] ), len(S) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_numpy_matrix( K * ( K >= T ) )\n",
    "y = np.zeros(K.shape[0])\n",
    "y[S] = 1\n",
    "nx.set_node_attributes(G, dict(enumerate(df[\"scores\"].values)), 'score')\n",
    "nx.set_node_attributes(G, dict(enumerate(df[\"tfisf\"].values)), 'tfisf')\n",
    "nx.set_node_attributes(G, dict(enumerate(df[\"btfisf\"].values)), 'btfisf')\n",
    "nx.set_node_attributes(G, dict(enumerate(df[\"lexrank\"].values)), 'lexrank')\n",
    "nx.set_node_attributes(G, dict(enumerate(df[\"target\"].values)), 'target')\n",
    "nx.set_node_attributes(G, dict(enumerate(df[\"R2.R\"].values / df[\"num_words\"].values)), 'R2.R')\n",
    "nx.set_node_attributes(G, dict(enumerate(df[\"R1.R\"].values / df[\"num_words\"].values)), 'R1.R')\n",
    "nx.set_node_attributes(G, dict(enumerate(1 - df[\"position\"].values / df[\"doc_sents\"].values)), 'rel_pos')\n",
    "nx.set_node_attributes(G, dict(enumerate(df[\"nouns\"].values)), 'nouns')\n",
    "nx.set_node_attributes(G, dict(enumerate(df[\"num_words\"].values)), 'words')\n",
    "nx.set_node_attributes(G, dict(enumerate(y)), 'summary')\n",
    "nx.write_graphml(G, group + \".graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>doc_sents</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>group</th>\n",
       "      <th>set</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>num_words</th>\n",
       "      <th>R1.R</th>\n",
       "      <th>R1.P</th>\n",
       "      <th>R2.R</th>\n",
       "      <th>R2.P</th>\n",
       "      <th>nouns</th>\n",
       "      <th>prpns</th>\n",
       "      <th>nf</th>\n",
       "      <th>lexrank</th>\n",
       "      <th>tfisf</th>\n",
       "      <th>btfisf</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>D0929</td>\n",
       "      <td>A</td>\n",
       "      <td>XIN_ENG_20050919.0201</td>\n",
       "      <td>22</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.5114</td>\n",
       "      <td>0.03288</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>()</td>\n",
       "      <td>1.023223</td>\n",
       "      <td>0.154728</td>\n",
       "      <td>0.761594</td>\n",
       "      <td>1.015507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>D0929</td>\n",
       "      <td>A</td>\n",
       "      <td>XIN_ENG_20050919.0201</td>\n",
       "      <td>22</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.01015</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>()</td>\n",
       "      <td>0.596115</td>\n",
       "      <td>-0.214641</td>\n",
       "      <td>-0.308914</td>\n",
       "      <td>0.989346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>D0929</td>\n",
       "      <td>A</td>\n",
       "      <td>XIN_ENG_20050919.0201</td>\n",
       "      <td>37</td>\n",
       "      <td>0.18000</td>\n",
       "      <td>0.5143</td>\n",
       "      <td>0.07328</td>\n",
       "      <td>0.2132</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>()</td>\n",
       "      <td>1.407246</td>\n",
       "      <td>0.956008</td>\n",
       "      <td>0.684440</td>\n",
       "      <td>1.007415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>D0929</td>\n",
       "      <td>A</td>\n",
       "      <td>XIN_ENG_20050919.0201</td>\n",
       "      <td>22</td>\n",
       "      <td>0.10510</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.04298</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>()</td>\n",
       "      <td>0.884419</td>\n",
       "      <td>-0.043430</td>\n",
       "      <td>0.199197</td>\n",
       "      <td>0.996231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>D0929</td>\n",
       "      <td>A</td>\n",
       "      <td>XIN_ENG_20050919.0201</td>\n",
       "      <td>15</td>\n",
       "      <td>0.07503</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.02023</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>()</td>\n",
       "      <td>0.436922</td>\n",
       "      <td>-0.786257</td>\n",
       "      <td>-0.825672</td>\n",
       "      <td>0.982294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      position  doc_sents  sent_id  group set                 doc_id  \\\n",
       "5922         0         10        0  D0929   A  XIN_ENG_20050919.0201   \n",
       "5923         1         10        1  D0929   A  XIN_ENG_20050919.0201   \n",
       "5924         2         10        2  D0929   A  XIN_ENG_20050919.0201   \n",
       "5925         3         10        3  D0929   A  XIN_ENG_20050919.0201   \n",
       "5926         4         10        4  D0929   A  XIN_ENG_20050919.0201   \n",
       "\n",
       "      num_words     R1.R    R1.P     R2.R    R2.P  nouns  prpns  nf   lexrank  \\\n",
       "5922         22  0.11260  0.5114  0.03288  0.1548      5      4  ()  1.023223   \n",
       "5923         22  0.08758  0.4167  0.01015  0.0500      6      2  ()  0.596115   \n",
       "5924         37  0.18000  0.5143  0.07328  0.2132      7      6  ()  1.407246   \n",
       "5925         22  0.10510  0.5000  0.04298  0.2125      5      6  ()  0.884419   \n",
       "5926         15  0.07503  0.5000  0.02023  0.1429      4      2  ()  0.436922   \n",
       "\n",
       "         tfisf    btfisf    scores  \n",
       "5922  0.154728  0.761594  1.015507  \n",
       "5923 -0.214641 -0.308914  0.989346  \n",
       "5924  0.956008  0.684440  1.007415  \n",
       "5925 -0.043430  0.199197  0.996231  \n",
       "5926 -0.786257 -0.825672  0.982294  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
